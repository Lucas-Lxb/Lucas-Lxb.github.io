[{"title":"MikPoly-编译优化论文解读","url":"/2024/10/15/MikPoly/","content":"    Optimizing Dynamic-Shape Neural Networks on\nAccelerators via On-the-Fly Micro-Kernel\nPolymerization \n\n\n\n\n   （基于微核聚合的加速器动态形状神经网络优化） \n\n一、涉及到的定义1.计算图计算图是神经网络中的一组算子及其数据依赖关系的表示，用于描述神经网络的前向和反向传播过程\n（1）计算图的类型：\n\n静态计算图\n在执行前先构建完整的计算图，然后通过给定的输入数据反复执行，计算图一旦定义，不能在运行过程中修改。\n\n\n​\t\t\t提前构建计算图后可以进行优化，如内存优化、操作融合等，能提高执行效率。适合推理阶段的高效执行\n\n动态计算图\n在执行过程中动态构建计算图，每次前向传播时计算图都会重新生成，因此可以根据具体的输入动态调整网络结构\n​\t相比静态图，动态图在某些场景下的性能优化空间较小，特别是在推理阶段，性能可能略逊，原因如图下：\n\n由于静态图的计算图在运行前就已经完整定义，因而框架可以从全局视角对图进行分析和优化，包括使用数据流分析、并行化处理、内存分配优化等复杂的优化操作\n由于动态图需要在每次在还行模型时动态生成，因而在优化时无法站在全局的视角，只能逐步构建，违法进行跨层操作融合\n动态图可以处理更加灵活的控制流结构（如 if 语句），但是因此也会带来额外的计算开销\n\n\n\n（2）为什么可以动态构建计算图？\n\n操作即构建：\n在 PyTorch 等框架中，计算图是在执行过程中通过每一步的操作动态构建的。例如，当你定义一个线性层操作或卷积层时，并不会立即创建整个计算图，而是在输入数据真正传递到该层时，框架会即时构建与这一操作相关的计算节点。\n\n计算图依赖于输入数据：\n动态图的计算不仅依赖于模型的结构，还依赖于输入数据的形状和内容。例如，递归神经网络（RNN）等网络在处理不定长的输入时，每次执行前向传播可能需要生成不同的计算图。\n\n执行与定义合二为一：在动态图框架中，计算图的定义与执行是一体的。每当执行一个操作，框架就在内部即时创建对应的计算图节点，并执行该操作。因此，计算图并不是预先定义好的，而是随代码的执行动态生成和处理。\n\n控制流：\n在动态图中，你可以使用 Python 中的控制流结构（如 if 语句或 for 循环）来动态改变网络的行为。因此，计算图并不是在模型定义时完全固定的，而是可以根据输入和条件语句在运行时动态调整。\n\n\n2.循环平铺结构将大循环分解成小块，使得每块的数据能够有效地放入CPU缓存中，减少对主存的访问次数 有助于提高局部性，因为缓存比主存的访问速度更快\n例子：\n#define N 1024double A[N][N], B[N][N], C[N][N];void matrix_multiply() {    for (int i = 0; i &lt; M; i++) {        for (int j = 0; j &lt; N; j++) {            C[i][j] = 0;            for (int k = 0; k &lt; K; k++) {                C[i][j] += A[i][k] * B[k][j];            }        }    }}\n\n应用循环平铺的版本：\n#define N 1024#define TILE_SIZE 32 // 假设这是一个合适的平铺大小double A[N][N], B[N][N], C[N][N];void tiled_matrix_multiply() {    for (int i = 0; i &lt; M; i += TILE_SIZE) {        for (int j = 0; j &lt; N; j += TILE_SIZE) {            for (int k = 0; k &lt; K; k += TILE_SIZE) {                for (int ii = i; ii &lt; i + TILE_SIZE; ii++) {                    for (int jj = j; jj &lt; j + TILE_SIZE; jj++) {                        for (int kk = k; kk &lt; k + TILE_SIZE; kk++) {                            C[ii][jj] += A[ii][kk] * B[kk][jj];                        }                    }                }            }        }    }}\n\n二、论文全文内容总结1 \tIntroduction三种张量计算的代表性方法该部分首先介绍了现有的三类支持高性能张量计算的代表性方法：\n\n供应商提供的自建库\n\n\n\n\n硬件平台\nx86 CPUs\nNvidia GPUs\n\n\n\n自建库\noneDNN\ncuBLAS\n\n\n上述自建库的实际实验效果有限，因为自建库针对特定形状进行优化的操作符实现并不适应所有形状，因而不可避免地导致性能下降。\n比如如下所示使用在cuBLAS实现的GEMM例程对于不同的张量形状有显著的性能差异：\n\n\n\n张量形状（M,N,K）\n(4096,4096,4096)\n(105,1024,12544)\n\n\n\n计算能力\n262.2 TFLOPS\n22.3 TFLOPS\n\n\n\n静态形状算子的张量编译器\n\n大多数张量编译器，如TensorFlow XLA [29]，TVM [7]和TC [55]，通过在大量搜索空间内搜索循环平铺结构来优化张量运算符，以确定给定形状的最佳实现。尽管如此，这些自动编译器在编译期间需要操作者的形状的先验知识。由于广泛搜索空间内的搜索成本很高，这种限制使得在动态场景中跨所有潜在形状优化张量运算符变得不可行。\n\n动态形状算子的张量编译器\n\n最近，一些研究探索了动态形状编译器[49，65，70]。一个例子是DietCode [65]，它通过优化形状通用搜索空间来增强传统的自动搜索器，以实现最佳操作符。然而，这些动态形状自动生成器仍然依赖于预定义的形状描述和离线代码优化。\n\n现有方法弊端和解决方案接下来对现有的方法的弊端进行阐述，并给出MIKPOLY的解决方案：\n\n现有的方法的弊端：利用了处理一系列形状的自动化程序来离线生成优化程序的有限子集。然而，这些自动排序器不能保证对预定义范围之外的形状的有效甚至正确执行，从而限制了它们在具有频繁形状变化的动态场景中的可用性。\n\nMIKPOLY的解决方案：创建一组微调的固定大小的微内核，每个微内核代表一个平铺的循环嵌套，负责执行张量运算符的一部分。这些微内核是离线生成的，并动态组合，为模型执行期间遇到的任何张量形状生成优化的代码。关键的挑战在于确定一个有效的组合策略，并在模型执行过程中以非常低的成本生成优化的代码。\n\n\n\n本文所做的工作根据该解决方案，文章阐述了本文所做的工作：\n\n提出了一个两阶段的方法来生成一个优化的张量程序\n引入了一个精确而轻量级的成本模型，有利于高效的在线聚合\n在代表性的两个加速器——GPU和NPU上进行测试，效果很好\n\n\n2\tBackground and Motivation背景：现实世界的应用程序通常表现出动态行为，例如语言建模中不同长度的句子，使得静态形状的神经网络不足。为了解决这一限制，动态形状神经网络已经被提出来支持更复杂的现实世界的智能应用。代表性场景如下：\n\n动态批次大小：较大批次会加快参数收敛的速度，并提高模型的稳定性。当前研究者提出动态批次大小的方法，随着训练过程动态调整批次大小，以提升性能和适应真实应用场景\n动态图像分辨率：传统的方法会将图像调整为固定尺寸，这样往往会导致信息的丢失，特别是在检测复杂场景中的小目标时。现代方法会通过引入高级池化方法，在不损失图像细节的前提下，提升测量精度。\n动态序列长度：在自然语言处理应用中，输入序列的长度是动态变化的，通常的解决方案是将序列填充到固定的最大长度，但是这种填充策略会浪费资源，特别是实际序列长度远小于最大序列长度的时候 ，因而要求使用优化的策略来动态处理序列长度，减少资源浪费，提高计算效率\n\n\n现有技术解决算子的实现\n\n\n静态形状张量编译器（如TVM）：\n\n功能：为固定形状的张量（M N K ）生成高效实现，通过枚举不同的平铺大小，找到最佳的配置，从而优化张量操作的性能\n问题：需要大量的离线编译时间，且只适用于特定的输入形状，另外在实际的动态形状任务中效率较低\n\n\n动态形状张量编译器（如DietCode）：\n\n功能：为不同形状生成多个预编译的张量程序，在运行时可以根据输入张量的形状进行选择，从而减少了编译开销\n局限性：要求在编译阶段事先了解可能的张量形状范围，从而限制了其应用范围\n\n\n\n\nMikpoly解决算子的实现(1)MikPoly 的核心思想\nMikPoly 主要目的是解决动态形状张量运算的挑战，特别是在不同形状的张量运算中保持高性能。\n通过一个两阶段的模板流程，MikPoly 在编译阶段生成了一组高度优化的微内核，并在运行时根据动态形状选择最优的微内核来执行张量运算。\n\n\n(2)两阶段流程\n离线阶段：在编译时，MikPoly 使用静态形状（如 M=4096,N=1024,K=4096）生成一系列的固定尺寸的微内核。这些微内核是通过探索最佳的 tile 参数和循环展开优化而生成的，并针对不同形状进行优化。\n在线阶段：在运行时，当具体的张量形状 M,N,K确定后，MikPoly 动态选择和组合这些预先优化好的微内核，生成最优的张量操作程序。在线阶段探索了不同的微内核组合策略来适应当前形状。\n\n(3) 微内核多样化\nMikPoly 提供了灵活的微内核生成和组合策略。例如，在离线阶段生成的微内核（如 micro-kernel(a.uM, a.uN, a.uK) 和 micro-kernel(b.uM, b.uN, b.uK)）被灵活地组织在一起，以应对不同的形状变化。\nPattern I 和 Pattern II 是两种不同的微内核聚合模式，它们根据不同的形状和性能需求进行适配选择。\n\n(4)成本模型的使用\nMikPoly 使用了一个精确且轻量化的成本模型，在运行时快速评估不同微内核组合的开销，并选择最佳组合，从而确保高效的执行。图中显示，成本模型指导了动态微内核的选择过程。\n\n(5)最终效果\nMikPoly 通过这种基于微内核的动态形状优化，有效地提升了动态形状神经网络在现代加速器上的性能，尤其是在形状变化频繁的情况下，MikPoly 显著缩短了编译时间，并且优化了运行效率。\n\n3\tThe MIKPOLY Design\nMikpoly的两阶段设计：微内核生成 (S1) 和微内核聚合 (S2)\n\nS1：微内核生成（Offline）：\t\n在编译时（离线），MikPoly 使用模板驱动的调优流程生成微内核。\n通过自动调度（Auto-Scheduling）来生成一组优化的微内核，每个微内核针对特定尺寸进行优化。\n同时开发一个微内核性能模型（Micro-Kernel Performance Model），为每个微内核提供性能预测。\n\n\nS2：微内核聚合（Online）：\n在运行时，基于已知的张量形状，MikPoly 通过运行时聚合组件（Runtime Polymerization）对张量操作的程序模板进行重新组织，生成不同的实现。\nMikPoly 使用一个轻量级的聚合成本模型（Polymerization Cost Model）选择最佳的微内核组合，以最小化计算开销。\n\n\n\n微内核生成阶段 (S1) 的工作流程：\n\nMikPoly 首先通过模板驱动生成微内核，并使用自动调度系统对其进行性能调优。\n针对不同的尺寸（形状）生成一组高度优化的微内核程序。\n该阶段的生成是离线进行的，因此不影响运行时性能。\n\n**运行时的微内核聚合阶段 (S2)**：\n\nMikPoly 在已知张量形状后，通过在运行时重新组织微内核来适应动态形状。\nMikPoly 使用轻量级成本模型，动态选择最适合的微内核组合，从而为每个特定的形状找到最佳的执行方案。\n这种方法能够在不同形状之间提供高效的切换，同时保持较低的计算开销。\n\n多级加速器抽象：\n\nMikPoly 的设计采用了多级加速器抽象（Multi-Level Accelerator Abstraction），将每个计算单元抽象为处理引擎（PE, Processing Engine），并使用局部内存和全局内存来表示不同层次的内存架构。\n这种抽象层次有助于 MikPoly 在硬件层面实现高效的资源利用，并提升神经网络的计算性能。\n\n 启发式策略探索：\n\nMikPoly 还探索了基于启发式的聚合策略，以根据运行时的张量形状选择最佳策略。通过多样化的策略选择，MikPoly 能够在不同的执行环境中高效运行。\n\n最终目标：MikPoly 通过结合离线微内核生成和在线微内核聚合，解决了动态形状张量操作中性能不稳定的问题，能够在现代加速器上显著提高动态形状神经网络的执行性能。\n多级加速器抽象Mikploy为硬件平台设计了多级加速器抽象，考虑指标：\n：硬件中的处理引擎（并行处理的计算单元）的数目（PEs）\n：单个PE内的本地存储器\n：多个PEs之间的共享存储器\n一个张量程序在硬件上运行的并行性依赖于，它的内存访问特性（独占或共享）由 和控制。在可行的情况下，用于存储数据，从而提高内存访问效率，而在PE之间平均分配带宽。\n两款代表性加速器的H指标如下图所示：\n\n两阶段优化该部分通过GEMM算子的动态形状优化后张量程序执行过程的例子对MIKPOLY的两阶段优化过程进行阐述。\n解耦优化空间==介绍循环平铺的概念==\n将平铺程序模版定义为Q，可将Q分为两个部分：和\n：专门用来充分利用本地存储器的最内层循环\n：为优化共享存储器而存在的剩余循环\n离线优化空间①使用中的离线循环作为微内核的模版，声明为\n②通过模版，生成一系列具有固定大小的微内核，并且在生成微内核的同时，为每个微内核生成对应的性能模型（在之后的微内核聚合的阶段中使用）\n在线优化空间①使用预先定义好的聚合模式（GEMM则为表3中的两种模式）为（外层循环）重新组织排列方式，再加上前一阶段已经生成完毕的中的微内核集合，一同为Q重新构建具体的实现方式。\n②对得到的所有程序通过探索所有固定大小微内核的聚合策略实现参数的实例化\n③在这些实例化的程序中根据成本模型选取性能最好的程序样例进行导出\n优化目标使用表示Mikpoly探索得出的所有张量程序的集合，那么寻找最优程序的问题可以被定义为优化问题：这个为本文定义的聚合成本模型，通过并行性、内存访问和资源利用等因素来估计其性能\n微内核生成自动调整固定大小的微内核将离线优化空间中使用一个模版生成的微内核的集合定义为,每个微内核定义为\n该阶段的运行过程如下：(三个超参数：、、)\n\n根据模版生成每维度的平铺大小在范围的微内核，将这些微内核统一包含在中\n生成一个张量程序，如下：\n\nfor i in range(dimension_size_1):    for j in range(dimension_size_2):        for k in range(dimension_size_3):            C[i, j] += A[i, k] * B[k, j]\n\n设置张量程序中的三个dimension_size的范围为，得到一系列测试程序\n\n使用第一步得到的微内核集合执行这些测试程序，按照运行过程中的平均性能进行排名，保留的性能表现者\n\n在实际的评估结果中，我们选择     = 32  = 12  = 40     作为实际参数，该组参数能够最小化离线微内核生成和在线聚合的开销\n微内核性能模型在微内核生成的过程中，Mikpoly会为每一个微内核生成一个性能模型，微内核定义为，性能评估模型定义为\n假设GEMM表示为  (M N K) = (t1 * uM,    t2 *uN,   t3 * uK)\n(1)\n得到单个时不能并行化（因为存在数据相关），因而应该在单个PE上执行\n（每个的计算会运行t3个的实例）\n对单个的计算采用流水线技术，该技术分为三个阶段 /单个流水线任务可分为三个阶段：\n\n加载数据到\n在单个PE上使用微内核处理的数据（中间结果存储在中）\n最终将结果从写回到\n\n(2)\n各个之间可通过在多个PE之间并行工作\n根据以上GEMM表示形式，如果使用微内核，则\n共有t1 * t2 个流水线任务，PE个数为，那么该GEMM算子的执行成本为t1 * t2 / 个流水线任务的成本\n(3)\n由于在运行时才会得到 t1 t2 t3,因而只考虑单个流水线任务即可\n定义性能模型函数，设定t∈[1,],在平台H上的单个PE上预先运行，得到一系列预先计算好的性能数据(  一般根据经验设定为 5120 )\n微内核聚合聚合模式在这个阶段，MIkpoly将划分为多个循环嵌套，每个循环嵌套包含相同的微内核，但是只处理原始区域的一部分(称为第 个区域)\nMikpoly将操作符GEMM的输出分为7块，具体如下图（a）所示，并依据这7块区域的不同组合方式设置出了9种不同的聚合模式。（通过大量的负载评估实验中得到）\n\n聚合策略使用在离线评估性能在前的微内核集合的中的微内核和前一步得到的聚合模式对整个聚合程序模版Q进行实例化，并且使用类似于CUTLASS的局部填充技术来最大限度减少边界检查，确保了能够填充任何形状的微内核组合的可能性。\n填充技术的原理如下图所示：\n聚合成本模型该模型考虑因素如下：\n\n微内核生成时的性能模型\n程序在实际执行过程中的并发性\n\n采用的成本模型如下：$$\\text{Cost}(S, H) = \\sum_{(R_i, \\tilde{K}i) \\in S} f{\\text{wave}}(R_i, \\tilde{K}i, H) \\times f{\\text{pipe}}(R_i, \\tilde{K}_i, H)$$\n$$f_{\\text{wave}}(R_i, \\tilde{K}i, H) = \\left\\lceil \\frac{f{\\text{parallel}}(R_i, \\tilde{K}i)}{|P{\\text{multi}}|} \\right\\rceil$$\n$$f_{\\text{pipe}}(R_i, \\tilde{K}i, H) = g{\\text{predict}} \\left( f_{\\text{num}}(R_i, \\tilde{K}_i), \\tilde{K}_i, H \\right)$$\n其中是单个流水线任务的执行开销\n则是并行执行多个流水线任务的成本（波的次数）\n是流水线任务的个数\n是离线阶段得到的性能模型函数\n$f_{\\text{num}}(R_i, \\tilde{K}i)是在某一嵌套循环$  R{\\text{i}}$$中，一个流水线任务中出现的微内核实例的个数\nMIkpoly完整算法实现\n\n离线生成阶段（Offline Generation）：\n\n在这个阶段，优化后的微核  是通过微核模板  使用 TVM 自动调度器生成的。\n\n动态多态化阶段（On-the-Fly Polymerization）：\n\n当运行时知道某个动态形状时，MikPoly 会尝试应用预定义的模式基于一个两阶段的模板 Q。\nMtkPoly 利用启发式方法来探索多态化策略，并估算代价。\n如果  的成本超过当前最佳策略的成本，那么相关的策略将会被跳过，从而显著缩小搜索空间并减少运行时的开销。\n\n最终结果：\n\nMtkPoly 基于最佳的多态化策略，构建了一个优化的张量程序 。\n\n4\t Implementation本节对不同的加速器平台的实现细节进行阐述。\nGPU 平台\n自动调度器和模板：\n对于 GPU，MtkPoly 使用 TVM 自动调度器结合 CUTLASS 模板，生成固定大小的参数化微核。生成的微核被编译成二进制文件，并保持恒定的形状大小，张量的起始地址和循环迭代次数在运行时作为参数动态确定。\n\n\n动态多态化：\n在运行时，MtkPoly 基于输入形状选择合适的多态化策略，并实例化对应的微核。对于 GPU，采用了模式 I 和 II，以保持性能与运行时开销之间的最佳平衡。线程块通过 GPU 硬件调度器动态分配给多处理器（SM），避免静态分配。\n\n\n性能优化：\n由于硬件的高效动态调度能力，GPU 上不需要静态分配线程资源，重点在于减少运行时的开销。因此，MtkPoly 仅使用少量预定义的模式（模式 I 和 II），以优化性能和运行时效率。张量地址偏移等信息在运行时传递给微核，并通过标量赋值，确保最小化的开销。\n\n\n\nNPU 平台\n手动模板和分配：\n对于 NPU，MtkPoly 使用手动模板生成固定大小的参数化微核。与 GPU 类似，生成的微核同样编译为二进制文件，并保持恒定的形状大小，同时将张量的起始地址和循环迭代次数作为运行时参数。\n\n\n动态多态化：\n在 NPU 上，MtkPoly 提供了更多的模式选择（模式 I-IX），并使用最大-最小静态分配算法将任务并行化分配到多个计算核心（如 DaVinci Cores）。这些模式为微核在多个处理单元之间的分配提供了灵活性和更高的并行性能。\n\n\n性能优化：\n在 NPU 平台上，由于更注重并行化执行的性能，手动分配的微核使用最大-最小静态分配算法来优化不同核心之间的任务负载分布，以提升整体性能。与 GPU 不同，NPU 的并行化任务需要手动调度和分配，因此 NPU 使用了更多的多态化模式来适应不同的任务。\n\n\n\n5\tEvaluationMikpoly的目标：能够有效优化加速器上的动态张量算子和神经网络，表现优于现有的水平\n模型评估主要聚焦于两个问题：\n\n是否能够优化动态张量算子和神经网络\n提出的成本模型是否能够有效地支持微内核聚合\n\n实验环境硬件平台：\n\n软件环境：\n\n\n\n平台\n软件环境\n性能评估工具\n\n\n\nGPU\nCUTLASS、CUDA toolkit(with cuBLAS and cuDNN libraries)\nPytorch for CNN models and   TuroTransformers for languanges models\n\n\nNPU\nCANN SDK\nMindSpore\n\n\n软件解释：\n\ncuBLAS and cuDNN在GPU上提供了GEMM和卷积运算的标准基础实现\nCUTLASS在GPU上提供了GEMM的模板化实现，用于加速GEMM和卷积运算操作\n\n备注：\n\n对上述的库中实现卷积操作的操作使用GEMM算子实现\n进行实验的预热操作，提前对程序运行20次\n\n基准测试针对GEMM算子和卷积算子进行测试，\n测试样例选择GEMM算子：\n\n\n测试样例分别来自百度的基准测试工具DeepBench，和真实世界中出现的GEMM算子的动态形状范围示例（包括基于Transformer的模型和基于CNN的模型）\n卷积算子：\n\n\n测试样例取自具有代表性的四类CNN网络模型\n测试过程将基础库（cuBLAS、cuDNN、CANN）中的标准GEMM算子和卷积算子替换成Mikpoly优化后的算子，对四种语言模型和四种CNN网络进行推理性能的测试\n性能结果对动态形状算子的优化效果MIKPOLY  vs  GPU libraries\n\n该表解读：\n\n定性来看：对于GEMM和卷积算子而言，Mikpoly、CUTLASS、cuBLAS（Mikpoly、CUTLASS、cuDNN）中效果最好的为Mikpoly\n定量来看：Mikpoly\n与cuBLAS相比，平均GEMM加速比为1.47倍（最大为4.82倍）\n与cuDNN相比，平均卷积加速比为1.98倍（最大为5.38倍）\n与CUTLASS相比，平均GEMM加速比和卷积加速比分别为3.02倍和1.72倍\n\n\n\nMIKPOLY  vs  NPU libraries\n\n该表解读：\n相较于CANN这个NPU厂商提供的自建库，Mikpoly的效果更好。其中GEMM和卷积的加速比分别为 1.10和1.41倍\n对动态模型推理的优化效果MiKPOLY对模型推理的延迟 = Mikpoly成本模型的计算时间(的耗时) + 优化后的算子在加速器上的最终执行时间\nGPU上进行模型推理（四种语言模型和四种CNN网络）\n\n测试主体：\n\nMikpoly\nCUTLASS\ncuBLAS / cuDNN（其效果作为基准线）\n\n\n\n\n对上述两表的解读：\n\n定性看：Mikpoly的效果在三者中是最好的；\n\n定量看：Mikpoly相对于cuBLAS / cuDNN对四种语言模型的加速比分别为1.39、1.38、1.36、1.37\n\n\n​\t\t\t  Mikpoly相对于cuBLAS / cuDNN对四种CNN模型的加速比分别为1.34、1.69、1.52、1.22\nNPU上进行模型推理和CANN相比，Mikpoly在四种CNN网络中的平均加速比为1.30、1.19、1.32、1.38\nMikpoly和现有技术对比所比较对象：\n\nMikpoly\n\n现有的动态形状张量编译器 DietCode、Nimble\n\n厂商自建库CUTLASS\n\n\n\n测试样例取自前文使用GEMM算子的1433个测试案例\n测试结果如下表所示：\n\n\n根据该表，Mikpoly相比于DietCode、Nimble、CUTLASS的加速比分别为2.94、7.54、3.59\n\n深入测试：测试四种语言模型的模型推理性能，实验结果如下表所示：\n\n通过该表得到Mikpoly相较于剩余三种工具中性能最优的DietCode的加速比平均为1.55\n\n根据实验得到的差异进行分析：因为动态编译器预先定义好了动态形状的变化范围，因为当实际的形状超出了它所定义的范围时，程序会因为越界错误或者资源不可用导致无效的错误。\n下面使用实验验证分析的正确性：\n\n根据上表，当预先设定DietCode输入范围为128、512、1024、2048、8192时，在整个的8192个测试样例中，其有效执行次数始终没有Mikpoly效果好，另外假设测试样例的范围就是DietCode的预先设定值时，它的运行效果仍旧没有Mikpoly效果好，这一点可通过下表得到。\n\n将Mikpoly应用到大语言模型LLM在这部分文章选用LLM的一个版本——Llama2-13b\n对比主体：\n\nMikpoly\nFaster Transformer(专门针对英伟达GPU的模型加速推理工具)\n\n测试结果：\n\n通过上表可以得到：在不同的batch size(1、2、4、8)和不同的输入序列长度下，Mikpoly相较于\nFaster Transformer的平均加速比为1.05  1.04  1.02  1.01\n性能分析该部分对GEMM算子在GPU上的完整性能进行分析\n在线聚合开销\n\n通过该表可知，Mikpoly在这三种GEMM实现工具中的总体运行时间是最短的（以cuBLAS为基线）\n另外，Mikpoly中的聚合成本（红色部分）占其总体运行成本的一小部分，并且由于高效的成本模型，这部分成本随着形状的增大而减少\n成本模型有效性为了证明成本模型的有效性，提出了三种Mikpoly的变体：\n\nMikpoly-ORACLE      在动态聚合阶段采用穷举搜索，最终只计算优化后的程序的运行时间，不考虑动态聚合时的时间成本\nMikpoly-WAVE           尽可能减少并行度，因而容易产生大尺寸的微内核\nMikpoly-PIPE             最大化单个流水线任务的性能，容易产生小尺寸的微内核\n\n对以上三种变体，再加上Mikpoly和CUTLASS一共五个测试主体（以Mikpoly-ORACLE 为基线），测试得到如下结果：\n\n\n实验结果：\n相较于Mikpoly-ORACLE，Mikpoly、Mikpoly-WAVE、Mikpoly-PIPE的加速比分别为0.96、0.81、0.72 \n结果分析：\n\nMikpoly考虑了减少并行度和最大化单个流水线任务的性能，因而在Mikpoly、Mikpoly-WAVE、Mikpoly-PIPE中性能最好。\n\n虽然Mikpoly-ORACLE的运行时性能最好，但是在线聚合阶段由于其采用了穷举搜索的方法，因而根据给定的动态形状到最终产生合适的聚合策略的时间长达1.6秒，而Mikpoly则只需要2微秒；同时从最终的运行时间看，Mikpoly-ORACLE只计算运行时的时间和Mikpoly聚合时间加上优化后程序运行时间大致相同，这说明了成本模型的有效性\n\n\n超参数分析本部分对前文在离线阶段微内核生成时涉及到的三个超参数进行探讨\n\n通过该表可以得到，随着三个超参数的增加，Mikpoly相对于cuBLAS的加速比逐渐上升，并最终达到饱和点，根据该图得到三个超参数的最终设置的值\n6 \tCase Studies在这部分探讨微内核聚合有效性的根本原因——提高了硬件利用率、缓解了负载不均衡的问题\n该部分考虑动态形状(M N K) = (4096,1024,4096),使用以下三种微内核组合对该算子进行优化：\n\n\n\n\n\n\n\n通过上图可以得到，的执行时间较更短，以下采用来和进行对比。\n在GPU上分别对这两种微内核组合进行测试，使用GPU的性能分析工具测试相关的测试指标，得到的结果如下：\n\n\n性能指标解释：\n\nsm_efficiency%：一个SM（流式处理器）上至少有一个wrap（线程组）处于活动状态的时间百分比\n\nelapsed_cycles_sm:每个SM上的始终周期数\n\ngrid_size:为GEMM任务分配的线程块的数量\n\n\n通过该表可知，在采用微内核聚合之后，sm的利用率上升，并且单个sm的时钟周期数目降低（任务完成时间减少了），这样就提高了硬件的利用率，最终的效果自然也就更好了\n\n除了使用性能评估工具对相关指标进行测试外，文章还对两种组合优化后的程序在实际硬件中的运行轨迹进行了建模，使用理论计算得出微内核聚合对负载不均衡的缓解问题。\n理论计算过程如下：\n\n使用GPU型号为A100，它含有108个SM，每个SM最多容纳64个wrap;\n\n在实际的运行过程中，两个kernel（、）对SM的占用率只有1 / 8，为8 wrap / SM;\n\n108 * 8 = 864，即每个波次该GPU只能处理864个wrap;\n\n\n对于GEMM-A =(256,128,32)：单个流水线任务需要256个线程的线程块（8 wrap）;一共所需要的流水线任务数目为（4096 * 1024） / （256 * 128） = 128个；则最终需要128 * 8 = 1024个wrap,需要 1024 / 864 = 2个波次才能完成\n\n对于GEMM-AB：按照模式二将GEMM算子划分为两部分：- （3072 1024 4096）    使用GEMM-A =(256,128,32)- （1024 1024 4096）    使用GEMM-B =(64,64,64)第一部分:\t由于使用GEMM-A，单个流水线任务需要256个线程的线程块（8 wrap）;\t一共所需要的流水线任务数目为（3072 * 1024） / （256 * 128） = 96个；\t则需要96 * 8 = 768个wrap第二部分:\t由于使用GEMM-B，单个流水线任务需要128个线程的线程块（4 wrap）;\t一共所需要的流水线任务数目为（1024 * 1024） / （64 * 64） = 256个；\t则需要256 * 4 = 1024个wrap将两部分进行加和，得到最终需要768 + 1024 = 1792个wrap,为1792 / 864 = 3个波次完成\n\n对上述的计算过程进行绘图，得到下图：\n\n根据该图，可以得到两个组合的执行时间分别为 (2 * Ta)  和  (Ta + 2 * Tb)\n其中Ta、Tb分别表示两个内核组合执行单个流水线任务的时间。\n在Ta &gt; 2 * Tb时，微内核聚合相对于单个微内核的效果为(2 * Ta) / (Ta + 2 * Tb) 倍。\n7\t Discussion通用性在GPU/NPU上对GEMM/卷积算子和语言模型和CNN网络的加速效果好\n适用性\n对输入形状在较大范围内频繁变化时的操作符表现良好\n在编译时已知某些输入形状的范围时，可通过更加合适的微内核集合的生成来细化成本模型，最终实现更好的优化来增强性能\n\n限制\n计划探索MikPoly与图级优化技术的结合，例如算子融合，以进一步增强动态形状场景中图级的性能\n当前的实现使用基于GEMM的卷积方法，研究其他卷积实现有潜在的好处，例如Winograd，它可能会提供额外的性能改进\n\n","tags":["编译优化"]},{"title":"RPC_communciation_mechanism on TVM","url":"/2024/09/07/RPC_communciation_mechanism%20on%20TVM/","content":"    RPC通信机制 \n\n一、概述RPC(remote procedure call)远程过程调用，是一种在不同计算机之间的进程间通信的机制。通过RPC，我们可以像调用本地函数一样调用远程函数，简化了分布式系统的开发和维护工作。\n二、RPC基本原理RPC的基本工作原理如下：\n\n调用方将调用请求封装成一个消息，并通过网络发送给远程服务\n远程服务接收到消息后，解析消息并调用相应的函数\n远程函数执行完毕后，将结果封装成消息并发送给调用方\n调用方接收到消息后，解析消息并获取结果\n\n\n三、TVM RPC 通信 (自定义RPC框架)1.TVM RPC通信核心组件\nRPC Tracker : 跟踪和管理远程设备，记录每个远程设备的状态和位置\nRPC Server : 在远程设备上运行，接受来自客户端的请求并执行任务\nRPC Client : 在本地运行，用于向远程设备发送任务，并接受执行结果\n\n2.RPC通信流程（1）启动 RPC TrackerRPC Tracker是负责管理所有远程设备的中心节点，它会追踪已经注册的 RPC Server，并允许客户端通过Tracker定位和连接到相应的设备。\n​\t启动 RPC Tracker的命令如下：\npython -m tvm.exec.rpc_tracker --host=0.0.0.0 --port=9190\n\n\n\n（2）启动 RPC Server​\t在远程设备上（如树莓派、GPU 服务器等），需要启动 RPC Server，它会注册到指定的 RPC Tracker 并等待客户端的任务请求。启动命令如下：\npython -m tvm.exec.rpc_server --tracker=tracker_host:9190 --key=raspberry_pi\n\n​\tRPC Server会注册自身到Tracker，成为可用的设备，并接受从客户端发来的任务请求。\n（3）客户端连接RPC Server​\t在本地机器上，客户端通过 RPC Tracker 查找和连接到远程设备（RPC Server）。客户端首先连接到 Tracker，然后通过指定的 key 请求连接到特定设备。\n（4）执行远程任务​\t一旦客户端连接到远程设备（通过 RPC Tracker），它可以在该设备上执行远程任务，例如推理或性能调优。可以在远程设备上加载模块并运行。\n（5）结果返回——（自动完成）​\t远程设备（RPC Server）在执行完任务后，会通过网络将结果返回给本地客户端（RPC Client）。这个过程是自动完成的，客户端无需关心底层的网络传输，TVM 的 RPC 框架会处理数据的序列化、传输和反序列化。\n3.TVM RPC的工作原理TVM 的 RPC 是基于序列化、网络通信和函数调用机制构建的，其工作原理可以分为以下几个步骤：\n\n序列化请求：当客户端请求调用远程设备上的函数时，RPC 框架会将函数名称、参数、设备标识等信息序列化为消息。\n网络传输：这些消息通过网络发送给远程设备（RPC Server）。RPC Server 监听指定端口，接收到消息后，解析请求。\n执行函数：RPC Server 解析出要调用的函数名称和参数，在设备上执行该函数。执行过程中使用远程设备的计算资源（如 CPU、GPU 等）。\n序列化结果：执行完成后，RPC Server 会将结果序列化，并通过网络返回给客户端。\n反序列化结果：客户端接收到结果后，将其反序列化为可用的 Python 对象，继续后续逻辑。\n\n4.TVM RPC常见的使用场景\n分布式调优：利用多个远程设备的资源进行分布式自动调优，如在树莓派、GPU集群上调优模型。\n异构计算：在不同的硬件设备（如CPU、GPU、FPGA等）上执行同一模型进行性能对比和优化。\n远程推理：将模型部署在远程服务器或嵌入式设备上，客户端通过RPC调用远程推理服务。\n\n","tags":["计算机网络"]},{"title":"test","url":"/2024/08/13/blog%E6%B3%A8%E9%87%8A/","content":"\n一、文章开头设置：文章 .md 文件最上方以 — 分隔的区域，用于设置文章的变量:\n1.password: 123456  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t设置文章访问密码  2.code_block_shrink:  false    \t\t\t\t\t\t \t\t\t\t\t\t\t\t\t代码块不自动收缩   3.excerpt: 这是《Keep 主题使用指南》的摘要信息......        \t\t\t\t\t\t\t\t\t给文章单独设置摘要信息    4.&lt;!-- more --&gt;   ：                                      \t\t\t\t\t\t\t\t\t&lt;!-- more --&gt; 之前的内容将会被视为摘要，  \t\t\t\t \t\t\t\t\t并在 Home（首页）将这段内容的 Markdown 排版格式完\t\t\t\t\t\t\t\t\t整显示出来\t\t\t\t 5.mathjax: true   \t\t\t\t\t \t\t\t\t\t\t\t\t    在文章中显示数学公式\n\n二、文章重要显示内容：\n    依然连仿真框架都没有, 真寒酸\n    我们之所以设置这部分的实验内容，是为了让大家知道...\n    \n        遇到系统性的bug，肯定消不出去...\n        离开了讲义，就什么都做不了...\n        一个很现实的场景是...\n    \n\n![image-20240814085831707](https://cdn.jsdelivr.net/gh/Lucas-Lxb/pictures-picgo/img/image-20240814085831707.png)\n\n\n    如果你想使用Chisel\n    \n        cd ysyx-workbench\n        bash init.sh npc-chisel\n    \n    上述命令会将 npc 目录中的文件换成一个Chisel开发环境...\n\n\n\n三、博客编写和部署命令hexo new title\n\nhexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy\n\n"},{"title":"String类的特性","url":"/2024/08/04/String%E7%B1%BB%E7%9A%84%E7%89%B9%E6%80%A7/","content":"问题描述​\t下面程序运行的结果是什么？尝试画出内存布局图。\n\n\npublic class StringExercise02 &#123;    String str = new String(&quot;lxb&quot;);    final char[] ch = &#123;&#x27;j&#x27;,&#x27;a&#x27;,&#x27;v&#x27;,&#x27;a&#x27;&#125;;    public void change(String str,char ch[]) &#123;        str = &quot;java&quot;;        ch[0] = &#x27;h&#x27;;    &#125;    public static void main(String[] args) &#123;        StringExercise02 ex = new StringExercise02();        ex.change(ex.str,ex.ch); //①        System.out.print(ex.str + &quot; and &quot;);        System.out.println(ex.ch);    &#125;&#125;\n\n\n解决方法一、方法执行时分析public void change(String str,char ch[]) &#123;        str = &quot;java&quot;;\t//代码②        ch[0] = &#x27;h&#x27;;\t//代码③    &#125;\n\n​\t在执行代码①处时，ex.str 和 ex.ch 被传递给change方法，由于二者均为引用数据类型(数组类型和String类型)，因而传入 change 方法时，会直接使得形参 str 指向ex对象的 str 和 ch 属性在堆中的地址，后开始执行该方法：\n由于代码②处将字符串常量赋值给 str，因而会在常量池中创建 “java” 对象，并将 str 指向该处于常量池中的对象，因而上述图中的虚线断裂，箭头重新指向常量池中的 “java” ；\n由于 ch 并未重新赋值，而是直接在原来的基础上对原数组进行修改，因而ch的指向不改变，但是该数组的内容发生改变。\n二、方法执行后分析​\t由于在执行该方法时 JVM 会在栈中创建 change 栈，在该方法执行完毕之后进行销毁，因此该方法的形参 str 和 ch 均被销毁，并返回主栈。此时 ex 对象的 str 属性并未改变，为 “lxb” ；ch 属性则发生变化，变为 “hava” ,因而最终输出为 lxb and hava.\n","categories":["码农搬砖"],"tags":["Java基础"]},{"title":"一生一芯","url":"/2024/08/13/%E4%B8%80%E7%94%9F%E4%B8%80%E8%8A%AF/","content":"一、Linux环境搭建1.gdb调试\n\n1.gdb -q Filename 进入gdb调试器\n2.（gdb）b 3\t\t\t在第三行添加断点\tbreakpoint\n3.(gdb) run\t\t\t\t执行程序\n4.(gdb)whatis iNum\t查看iNum的数据类型\n5.(gdb) c\t\t\t\t\t继续执行程序（直到下一个断点或者之后没有断点就运行完整个程序）\n6.(gdb)\tn\t\t\t\t一步一步执行程序\n7.(gdb) p iNum\t\t输出iNum值\t\t\t\t\t\t\t（gdb中变量的值是每一步执行之前的数字）\n2.Makefile从源码到可执行文件的四个阶段：\n\n预处理(.c\t-&gt;\t.i)\ngcc -E test.c -o test.i\n\n\t\n\n编译   (.i     -&gt;    .s)                 得到汇编语言代码\ngcc -S test.i -o test.s\n\n\n汇编   (.s    -&gt;    .o)                 得到机器语言代码\ngcc -c test.s -o test.o\n\n\n链接   (.o    -&gt;    )                    得到可执行文件\ngcc test.o -o test\n\n\n\ngcc(选项)(参数)：\n​\t-o：指定生成的输出文件；​\t-On：n为数字1~3，使用编译优化级别n编译程序；​\t-E：仅执行编译预处理；​\t-S：将C代码转换为汇编代码；​\t-Wall：显示警告信息；​\t-c：仅执行编译操作，不进行链接操作。\n3.tmux使用Ctrl\t+\tb,\tshift\t+\t%\t新建左右窗格\nCtrl\t+\tb,\tshift\t+\t“\t新建上下窗格\nCtrl\t+\tb,\tx\t删除当前窗格\n4.编译nemu的坑​\t\t\t\t\t\tmake menuconfig命令后出现如下情况：\n/home/lixuanbo/Desktop/ysyx/ysyx-workbench/nemu/scripts/config.mk:20: Warning: .config does not exists!/home/lixuanbo/Desktop/ysyx/ysyx-workbench/nemu/scripts/config.mk:21: To build the project, first run &#x27;make menuconfig&#x27;.\n\nCC confdata.cCC expr.cCC preprocess.cCC symbol.cCC util.cYACC build/parser.tab.hmake[1]: bison: 没有那个文件或目录make[1]: *** [Makefile:27：build/parser.tab.h] 错误 127make: *** [/home/lixuanbo/Desktop/ysyx/ysyx-workbench/nemu/scripts/config.mk:39：/home/lixuanbo/Desktop/ysyx/ysyx-workbench/nemu/tools/kconfig/build/mconf] 错误 2\n\n解决方法：要安装词法分析和语法分析工具\tflex和bison\n二、搭建verilator仿真环境1.【一生一芯】搭建verilator仿真环境 - 老吴家的小阿哲 - 博客园 (cnblogs.com)2.verilator探幽(1)一个简单的例子1.将verilog代码写入文件top.v\n2.将**C++**代码写入文件sim_main.cpp\n3.使用下面的命令来运行Verilator:\nverilator --cc --exe --build -j 0 -Wall sim_main.cpp top.v\n\n4.使用\t.&#x2F;obj_dir&#x2F;Vtop\t来运行Verilator生成的可执行程序\n(2)稍微复杂的例子在Verilog中，assign 是一个关键字，用于为信号赋值\n(一)在C++代码中设置跟踪，创建波形文件\n\n1.编写top.v:\nmodule top (    input a,    input b,    output f);    assign f = a ^ b;endmodule\n\n2.编写main.cpp：\n#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;memory.h&gt;#include &lt;assert.h&gt;#include &quot;verilated_vcd_c.h&quot; // 生成vcd文件使用#include &quot;Vtop.h&quot;#include &quot;verilated.h&quot;int main (int argc, char **argv) &#123;    if (false &amp;&amp; argc &amp;&amp; argv) &#123;&#125;    const std::unique_ptr&lt;VerilatedContext&gt; contextp&#123;new VerilatedContext&#125;; //创建一个动态分配的 VerilatedContext 对象，并使用 std::unique_ptr 来确保在不再需要时自动释放内存    std::unique_ptr&lt;Vtop&gt; top&#123;new Vtop&#123;contextp.get()&#125;&#125;;    contextp-&gt;commandArgs(argc, argv);    contextp-&gt;traceEverOn(true); // 生成波形文件使用，打开追踪功能    VerilatedVcdC* ftp = new VerilatedVcdC; // vcd对象指针    top-&gt;trace(ftp, 0); // 0层    ftp-&gt;open(&quot;wave.vcd&quot;); //设置输出的文件wave.vcd    int flag = 0;    while (!contextp-&gt;gotFinish() &amp;&amp; ++flag &lt; 20)     &#123;        int a = rand() &amp; 1;        int b = rand() &amp; 1;        top-&gt;a = a;        top-&gt;b = b;        top-&gt;eval();        printf(&quot;a = %d, b = %d, f = %d\\n&quot;, a, b, top-&gt;f);        assert(top-&gt;f == (a ^ b));\t//验证top-&gt;f == (a ^ b)，若结果为假，程序终止，并输出错误信息        contextp-&gt;timeInc(1); // 时间+1，推动仿真时间         ftp-&gt;dump(contextp-&gt;time()); // dump wave    dump 方法通常用于将当前时间的信号状态写入到波形文件中    &#125;    top-&gt;final();    ftp-&gt;close(); // 必须有    return 0;&#125;\n\n3.使用如下命令：\nverilator --cc --exe --build -Wall --trace top.v main.cpp\n\n4.执行生成的Vtop可执行文件\n./obj_dir/Vtop\n\n5.shell观察波形\ngtkwave wave.vcd\n\n\n\n(二)在Verilog代码中设置跟踪，创建波形文件\n\n1.top.v:\nmodule top (    input a,    input b,    output f);    assign f = a ^ b;initial begin    if ($test$plusargs(&quot;trace&quot;) != 0) begin\t\t\t//在仿真开始时检查命令行参数，如果存在 &quot;trace&quot; 参数，则设置波形文件并将相关的信号状态写入到波形文件中         $display(&quot;[%0t] Tracing to wave.vcd...\\n&quot;, $time);         $dumpfile(&quot;wave.vcd&quot;);         $dumpvars();      end      $display(&quot;[%0t] Model running...\\n&quot;, $time);   endendmodule\n\n2.main.cpp:\n#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;memory.h&gt;#include &lt;assert.h&gt;#include &quot;Vtop.h&quot;#include &quot;verilated.h&quot;int main (int argc, char **argv) &#123;    if (false &amp;&amp; argc &amp;&amp; argv) &#123;&#125;    const std::unique_ptr&lt;VerilatedContext&gt; contextp&#123;new VerilatedContext&#125;;    std::unique_ptr&lt;Vtop&gt; top&#123;new Vtop&#123;contextp.get()&#125;&#125;;    contextp-&gt;commandArgs(argc, argv);    contextp-&gt;traceEverOn(true); // 生成波形文件使用，打开追踪功能    int flag = 0;    while (!contextp-&gt;gotFinish() &amp;&amp; ++flag &lt; 20) &#123;        int a = rand() &amp; 1;        int b = rand() &amp; 1;        top-&gt;a = a;        top-&gt;b = b;        top-&gt;eval();        printf(&quot;a = %d, b = %d, f = %d\\n&quot;, a, b, top-&gt;f);        assert(top-&gt;f == (a ^ b));        contextp-&gt;timeInc(1); // 时间+1，推动仿真时间    &#125;    top-&gt;final();    return 0;&#125;\n\n3.使用如下命令：\nverilator --cc --exe --build -Wall --trace top.v main.cpp\n\n4.执行生成的Vtop可执行文件\n./obj_dir/Vtop\n\n5.shell观察波形\ngtkwave wave.vcd\n\n三、Linux入门教程1.基本命令\nfind . -name “*.[c]” ———查找当前目录下的.c文件\n\ngrep “\\bint i\\b”  a.c   查找文件中定义变量i的位置\t\t\t—————-模式”\\bint i\\b”包含了单词边界锚点（\\b）在”int i”周围。这些锚点指定”int i”应该是一个完整的单词，而不是更大单词的一部分。\n\nwc     a.c  返回该文件的行数\t单词数\t字符数\t\t\t\t要求为文本文件\n\ncp\tsource\tdestination     复制命令      把左边文件的内容复制到右边\n\n\n2.Linux\tvs.\tWindows1.有些简单的事Windows GUI反而做不好比较两个文件是否相同\nLinux的解决方案：\n\n文本文件的解决方案\nvimdiff\tfile1\tfile2 \n\n非文本文件的解决方案\ndiff\tfile1\tfile2\n\n很大的文件\nmd5sum\tfile1\tfile2\n\n2.有些复杂的事Windows GUI几乎做不了列出一个项目中所有被包含的头文件\nLinux的解决方案：通过管道进行解决\nfind\t.\t-name  &quot;*.[ch]&quot;\t|\txargs\tcat\t|\tgrep  &quot;^#include&quot;  |\tsort\t|\tuniq\n\n3.工具如何运行(1)一个重要的踪迹工具：strace\nsystem\tcall strace,记录程序运行过程中的系统调用信息\n系统调用：一个由操作系统来执行的特殊的函数调用\n\n在Linux中,Path是一个非常重要的概念,它是用来定位系统中各种可执行文件、命令和脚本的路径。\nstrace bash -c &quot;PATH = &#x27;aaa:bbb:ccc&#x27; ls&quot;\t\t//整个命令的目的是使用strace追踪Bash shell在将PATH环境变量设置为&#x27;aaa:bbb:ccc&#x27;后执行ls命令时所发起的系统调用。\n\n(2)用户与man交互在 man 命令下进行手册的阅读时，可以使用一些快捷键和搜索命令来进行快速搜索。以下是一些建议：\n使用/进行搜索：\n\n- 在 `man` 页面中按 `/` 键，然后输入你要搜索的关键词，按回车。`man` 会定位到第一个匹配的文本。\n- 按 `n` 键继续查找下一个匹配，按 `N` 键查找上一个匹配。\n  ### (3)强大的shell1.通配符\t*\t（任意长度的任意字符串）\t|\t？\t（任意一个字符）\t|\t[...](集合中的任意一个字符)2.括号扩展&#123;...&#125;（例：echo Hello-&#123;a,bb,ccc&#125;-&#123;1,2&#125;!）---------&gt;------&gt;输出结果为：Hello-a-1! Hello-a-2! Hello-bb-1! Hello-bb-2! Hello-ccc-1! Hello-ccc-2!### (3)使用alias为常用命令设置别名```calias ls = &quot;ls --color&quot;    可以写入~/.bashrc文件，打开终端时生效，无需重复设置：source ~/.bashrc\n\n\n\n(4)任务管理1.查看任务管理器：ps aux\t\t\t\t——-&gt;   静态命令，只显示在命令运行时的瞬时数据。\n\nps 是一个用于报告当前系统进程信息的命令\naux 选项表示以详细的方式显示所有用户的所有进程\n提供了基本的进程信息，如进程ID（PID）、CPU利用率、内存使用、启动时间等\n\n\n\n2.top 提供了实时动态更新的系统监视器\n3.htop 是 top 的增强版，提供了更直观的界面和更多功能\n(5)输入输出重定向已知Linux上的程序在运行时默认打开了3个文件，通过“文件描述符”来编号：\n\n0号文件 - 标准输入（默认为当前终端）\n1号文件 - 标准输出（默认为当前终端）\n2号文件 - 标准错误（默认为当前终端）\n\n1.向文件追加输出\nls &gt;&gt; result.txt\n\n2.将标准错误重定向到文件\nls 2&gt; /dev/null...\n\n3.将标准输入重定向到文件，无需手动输入\nsort &lt; result.txt\n\n(6)管道：工具间组合的秘诀管道 &#x3D; 一个用于连接程序间输入输出的缓冲区\nxargs:一个特殊的命令，可以将标准输入转变为命令的参数\n\n自制CPU主频监视器\n\nwatch -n 1 &quot;cat /proc/cpuinfo | grep MHz | awk\t&#x27;&#123;print \\$1\tNR\t\\$3\t\t\\$4\t\t\\$2&#125;&#x27;&quot;\n\nLinux中可执行文件为ELF文件\n4.如何学习Linux最重要的Linux命令：\nmanman man - 学习如何RTFMman 3 printf\t-\t学习如何使用库函数printfman -k xxx\t检索含有关键字含有xxx的命令\n\n\n","tags":["Linux"]},{"title":"计算理论导引","url":"/2024/03/18/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA%E5%AF%BC%E5%BC%95/","content":"第一部分\t自动机与语言\n\n\n第一章\t正则语言1.1有穷自动机1.1.1 \t有穷自动机的形式化定义有穷自动机是一个5元组（Q，Σ，δ，q0，F）\n\nQ是一个有穷集合，称为状态集\n\nΣ是一个有穷集合，称为字母表\n\nδ：Q×Σ-&gt;Q，是转移函数\n\nq0∈Q，是起始状态\n\nF含于Q，是接受状态集\n\n\n1.1.2\t计算的形式化定义设M &#x3D; （Q，Σ，δ，q0，F）是一台有穷自动机，W &#x3D; W1W2W3…Wn是一个字符串并且其中任意wi是字母表Σ的成员。如果存在Q中的状态序列r0,r1,…,rn,满足下述条件：\n\nr0 &#x3D; q0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t机器从开始状态开始\nδ（ri,W(i + 1)） &#x3D; r(i + 1),i &#x3D; 0,…,n - 1            机器按照转移函数从一个状态到一个状态\nrn∈F                                                                如果机器结束在接受状态，则接受它的输入\n\n则M接受串W\n如果A &#x3D; {W | M接受W}，则称M识别语言A\n如果一个语言被一台有穷自动机识别，则称它为正则语言。\n1.1.3\t设计有穷自动机\n给每一种可能的字符串信息设计一个状态\n通过观察如何根据读到的符号从一种可能性到另一种可能性来设计转移\n接下来，把起始状态设置为对应于到现在为止还没有看到与任何符号（空串ε）相关联的可能性的状态\n\n1.1.4\t正则运算设A和B是两个语言，定义正则运算并、连接和星号如下：\n\n并：A∪B &#x3D; {x | x∈A或x∈B}\n连接：A▪B &#x3D; {xy | x∈A且y∈B}\n星号：A* &#x3D; {x1x2x3…xk | k &gt;&#x3D; 0 且每一个xi∈A}          ——&gt;一元运算\n\n一般来说，如果把某种运算应用于一个对象集合的成员得到的对象仍在这个集合中，则称这个对象集合在该运算下封闭。\n1.2\t非确定性非确定性是确定性的推广-&gt; 因此每一台确定性有穷自动机自动地是一台非确定性有穷自动机\nNFA与DFA的不同点：\n\n在NFA中，一个状态对应于字母表中的每一个符号可能有0个、1个或多个射出的箭头\n在DFA上，转移箭头上的标号是取自字母表中的符号；NFA中则可以取自ε\n一般来说，NFA的箭头可以标记字母表中的符号或ε。从一个状态可能射出0个、1个或多个带有标号ε的箭头\n\n可以把非确定性看做是若干独立的“过程”或“线程”，即能同时运行的一类**并行运算**，如果这些子过程至少有一个接受，那么整个计算接受。\n1.2.1 \t非确定性有穷自动机的形式化定义非确定性有穷自动机是一个5元组（Q，Σ，δ，q0，F）\n\nQ是一个有穷集合，称为状态集\nΣ是一个有穷集合，称为字母表\nδ：Q×（Σ∪{ε}）-&gt;（Q的幂集），是转移函数\nq0∈Q，是起始状态\n\n1.2.1\tNFA与DFA的等价性\n\n","categories":["理论知识"],"tags":["计算理论"]},{"title":"树莓派opencv-python踩坑记录","url":"/2024/04/04/%E6%A0%91%E8%8E%93%E6%B4%BEopencv-python%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/","content":"1.’xcb’插件检测得到但是不可加载：大概率是opencv与python版本冲突修改为：安装版本为 python3.8 与 Opencv-python4.1.2.\n\n\n\n\n2.使用pip安装软件包时出现SSL验证失败问题：pip版本在20.3以上时会出现此错误修改：\n法一:下载pip版本为20.2.4\npython -m pip install pip==20.2.4 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.compython -m pip install pip==20.2.4 -i http://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com\n\n法二：修改pip源\n修改为（以下任意一项） ———————–由于网速限制可能要多试几遍（我试了很多遍…….）\n\nhttp://mirrors.aliyun.com/pypi/simple/\n- ```  http://pypi.douban.com/simple\n\n\n\n注：树莓派所安装操作系统为DeBian 11.\n3.apt-get修改国内源（23.10 mantic版本）\n\n\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-backports main restricted universe multiversedeb http://security.ubuntu.com/ubuntu/ mantic-security main restricted universe multiverse# deb-src http://security.ubuntu.com/ubuntu/ mantic-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-proposed main restricted universe multiverse# # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-proposed main restricted universe multiverse\n\n\n无法获得锁：&#x2F;var&#x2F;lib&#x2F;dpkg&#x2F;lock-fronted\n\n解决方案:\nsudo rm /var/lib/dpkg/lock-frontendsudo rm /var/lib/dpkg/locksudo rm /var/cache/apt/archives/lock\n\n都运行一遍，具体也不知道哪条起了作用。\n出现以下错误：\nError:Could not build wheels for opencv-python ,which is required to install pyproject.toml-based projects------&gt;安装poetry------&gt;运行poetry init命令  #获得pyproject.toml文件------&gt;运行poetry install命令 #得到项目内文件的依赖\n\nPoetry 是一个用于管理 Python 项目依赖、虚拟环境和打包发布的工具。它通过一个名为 `pyproject.toml` 的配置文件来管理项目。使用 Poetry，你可以方便地定义项目的依赖关系、版本约束和脚本命令，并且它提供了一组命令来执行各种常见任务，例如安装依赖、创建虚拟环境、构建项目等。Poetry 是一个用于管理 Python 项目依赖、虚拟环境和打包发布的工具。它通过一个名为 pyproject.toml 的配置文件来管理项目。使用 Poetry，你可以方便地定义项目的依赖关系、版本约束和脚本命令，并且它提供了一组命令来执行各种常见任务，例如安装依赖、创建虚拟环境、构建项目等。以下是一些常用的 Poetry 命令：poetry init 是一个用于初始化 Python 项目的命令，它使用 Poetry 工具。Poetry 是一个 Python 包管理工具和项目管理工具，它可以帮助你管理项目依赖关系和构建工具。#当你运行 poetry init 命令时，它会引导你完成创建一个新的 Python 项目的过程。它会询问一些问题，如项目名称、版本、作者、项目依#赖等，并生成一个 pyproject.toml 文件，用于描述你的项目和其依赖关系。poetry new: 创建一个新的 Python 项目。poetry install: 安装项目的依赖。poetry add: 添加新的依赖项到项目。poetry remove: 从项目中移除依赖项。poetry update: 更新项目的依赖项。poetry run: 在虚拟环境中运行项目中定义的脚本命令。poetry shell: 进入项目的虚拟环境。poetry build: 构建项目。poetry publish: 发布项目到包索引。poetry config: 配置 Poetry 的设置。\n\n","categories":["折腾Linux"],"tags":["树莓派"]}]